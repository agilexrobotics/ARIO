# ARIO
Project link: [ario-data.github.io](http://ario-data.github.io)

Embodied AI is the current cutting-edge direction of intelligence science and is considered the only channel to achieve general AI. Developed countries led by the United States and many technology giants represented by OpenAI and Nvidia all regard embodied intelligence as an important strategic development and have begun their deployment with huge investment.

In recent years, robot technology has developed rapidly, profoundly affecting our production and daily life. From industrial manufacturing to family life, from deep-sea exploration to space experiments, robots are appearing more and more frequently in various fields. Robots are physical carriers of embodied intelligence. Embodied intelligence further endows robots with brains, senses, and experience capabilities, enabling them to continuously learn and improve through repeated interactions with the environment. The embodied intelligent robot system will subvert the traditional single working mode of dedicated and weakly interactive robots, and is expected to be implemented on a large scale in open and unstructured scenarios to meet the diverse needs of humans.

In the era of large models, Scaling Law is as important as Moore's Law. By increasing the amount of data and expanding the model scale, model performance can be continuously improved. In order to build a large basic model of embodied AI and build an application platform with large-scale effects for embodied AI, it’s urgent to create an open source large-scale and high-quality robot perception and operation data set.

Just as Stanford University's ImageNet promotes computer vision research, as an important part of the country's strategic scientific and technological strength, we hope to rely on the "China Computing Network" and "Qizhi" open-source ecology to take the lead in creating an open-source dataset with the same influence in the field of embodied AI: OpenRobots.

At present, Google (US) has firstly released the Open X-Embodiment data sets and based on which an Embodied AI control basic model RT-X is trained. It shows good generalization performance across scenarios, multi-tasks, cross-platforms, etc. and generally exceeds previous skill levels based on specific scenarios and data sets.

In order to promote cutting-edge exploration and industrial application in the field of embodied intelligence in China, Pengcheng and AgileX Robotics advocate that universities, scientific research institutes, enterprises and other related industries across the country take active actions to jointly create the first large-scale, multi-modal in China: ARIO(All robots in one), an embodied intelligence data set covering multiple scenarios, skills, tasks, and platform types. Although Open X-Embodiment has made a step ahead, it still has some shortcomings. For example, the perception data source is relatively single, including only images. Some data are not standardized, and most robot forms are single-armed, which limits downstream applications.

Compared with Open X-Embodiment, OpenRobots will be the world’s first data set to include five modalities (Image, point cloud, text, touch and hearing), covering both vertical fields such as service and industry and supporting rich application scenarios. Hence, we call on all parties to:
1. Establish unified collection and open-source standards for robot perception and operation data to ensure data standardization.
2. Strengthen data security and privacy protection to ensure that open-source data does not infringe personal privacy and business secrets.
3. Encourage universities, scientific research institutions and enterprises to actively participate in data collection, sharing and collaborative research, jointly build a high-quality data set ecosystem, and jointly promote the innovative development of embodied intelligence technology.

Let us work together to promote the creation of ARIO, and to contribute knowledge and strength to the prosperity and development of the embodied intelligence and robotics industry!
